#!/bin/bash

# NAMESCHEME is the name of the head node, and all NAMESCHEMEXX are compute nodes.

LOGFILE="/var/log/clic.log"
TWIDDLE_TIME=15
SLEEP_TIME=10
MAX_NODE_NUM=99

mainLoop() {
	while [ "true" ]; do
		# Queued and running jobs are one line each containing the job number
		rJobs=`qstat -r | tail -n+6 | awk '{print $1}'`
		qJobs=`qstat -i | tail -n+6 | awk '{print $1}'`

		nodesWanted=0
		nodesUp=0
		nodesBooting=0
		for node in $(seq -w 1 $MAX_NODE_NUM); do
			if [ "`state $node`" == "R" ]; then
				nodesUp=`expr $nodesUp + 1`
			elif [ "`state $node`" == "C" ]; then
				nodesBooting=`expr $nodesBooting + 1`
			fi
		done
		for jobNum in $qJobs; do
			# Keep track of time jobs have been queued
			if [ -z ${job[$jobNum]} ]; then job[$jobNum]=0; fi
			job[$jobNum]=`expr ${job[$jobNum]} + $SLEEP_TIME`
			# Determine if we need another node
			if [[ "$nodesBooting" -eq 0 && ( "${job[$jobNum]}" -gt "$TWIDDLE_TIME" || "$nodesUp" -eq 0 ) ]]; then
				nodesWanted=`expr $nodesWanted + 1`
			fi
		done
		nodesIdle=`numIdle`
		echo "`date +%H:%M:%S` Nodes wanted: $nodesWanted, Nodes up: $nodesUp, Nodes idle: $nodesIdle Nodes booting: $nodesBooting" | tee -a $LOGFILE
		# Check that the nodes wanted aren't currently booting (nodesWanted + jobsRunning > nodesUp)
		nodesWanted=$((($nodesWanted - $nodesBooting + 1) / 2))
		if [ $nodesWanted -gt 10 ]; then nodesWanted=10; fi #cap it at 10
		nodesWanted=$(($nodesWanted - $nodesIdle))
		create $nodesWanted

		# Check for nodes to delete
		numIdle=`numIdle`
		if [ -z "$qJobs" ] && [ "$numIdle" -gt "0" ]; then
			# Node(s) sit idle...
			if [ -z "$emptyTime" ]; then
				emptyTime=`uptime`
			elif [ "$(expr $(uptime) - $emptyTime)" -gt "$TWIDDLE_TIME" ]; then
				# ...and have sat idle for some time
				numToDelete=$((($numIdle + 1) / 2))
				delete $numToDelete
				emptyTime=""
			fi
		elif [ -n "$emptyTime" ]; then
			# Clean up
			emptyTime=""
		fi

		# Book keeping
		# States : <C|R|D>
		# C=creating, R=running, D=deleting
		# Null string for deleted nodes
		nodenamesUp=`upNodes` # As reported by slurm
		googleReported=`googleReportedUp` # Nodes that we think are up and aren't included here aren't actually up
		for node in $(seq -w 1 $MAX_NODE_NUM); do
			# Routine stuff
			if [ "`state $node`" == "C" ] && [ -n "`grep NAMESCHEME$node <<< $nodenamesUp`" ] && [ -n "`grep NAMESCHEME$node <<< $googleReported`" ]; then
				# Node was creating, now is running
				echo "Node NAMESCHEME$node is up" | tee -a $LOGFILE
				setState $node R
				updateIP $node # If the node's ip address changed, then slurm needs to be restarted
			elif [ "`state $node`" == "D" ] && [ -z "`grep NAMESCHEME$node <<< $nodenamesUp`" ] && [ -z "`grep NAMESCHEME$node <<< $googleReported`" ]; then
				# Node was deleting, now is gone
				echo "Node NAMESCHEME$node went down" | tee -a $LOGFILE
				sudo scontrol update nodename=NAMESCHEME$node state=down reason="Deleted"
				setState $node ""
			fi
			
			# Error conditions
			if [ "`state $node`" == "R" ] && [ -z "`grep NAMESCHEME$node <<< $googleReported`" ]; then
				# We think the node is running, but Google doesn't
				echo "ERROR: Node NAMESCHEME$node deleted outside of clic!" | tee -a $LOGFILE
				sudo scontrol update nodename=NAMESCHEME$node state=down reason="Error"
				setState $node ""
			elif [ "`state $node`" == "R" ] && [ -z "`grep NAMESCHEME$node <<< $nodenamesUp`" ] && [ -n "`grep NAMESCHEME$node <<< $googleReported`" ]; then
				# We think it should be running, and so does Google, but Slurm doesn't
				echo "ERROR: Node NAMESCHEME$node is unresponsive" | tee -a $LOGFILE
				deleteInstance $node
			elif [ -z "`state $node`" ] && [ -n "`grep NAMESCHEME$node <<< $nodenamesUp`" ] && [ -n "`grep NAMESCHEME$node <<< $googleReported`" ]; then
				# Node is running, but isn't registered. Maybe the script was restarted?
				echo "ERROR: Encountered unregistered node NAMESCHEME$node, subsuming into cluster" | tee -a $LOGFILE
				sudo scontrol update nodename=NAMESCHEME$node state=resume
				setState $node R
			elif [ "`state $node`" == "C" ] && [ "`timeInState $node`" -gt 200 ]; then
				# In create state, and taking way too long
				echo "ERROR: Node NAMESCHEME$node hung on boot" | tee -a $LOGFILE
				echo Y | gcloud compute disks delete NAMESCHEME$node &> /dev/null &
				deleteInstance $node
			fi
		done
		
		sleep $SLEEP_TIME
	done
}

uptime() {
	cat /proc/uptime | awk '{print $1}' | cut -d '.' -f 1 # In seconds, rounded down
}

numIdle() {
	local num="`sinfo -h -r -o %A | cut -d '/' -f 2`"	
	if [ -z "$num" ]; then
		echo 0
	else
		echo $num
	fi
}

upNodes() {
	sinfo -h -N -r -o %N
}

googleReportedUp() {
	gcloud compute instances list | tail -n+2 | awk '{print $1}'	
}

existingDisks() {
	gcloud compute disks list | tail -n+2 | awk '{print $1}'
}

state() {
	local node="`echo $1 | sed 's/^0*//'`"
	echo ${nodes[$node]} | awk '{print $1}'
}

timeInState() {
	local node="`echo $1 | sed 's/^0*//'`"
	expr `uptime` - `echo ${nodes[$node]} | awk '{print $2}'`
}

setState() {
	local node="`echo $1 | sed 's/^0*//'`"
	local state="$2"
	if [ -n "$state" ]; then
		nodes[$node]="$state `uptime`"
	else
		nodes[$node]=""
	fi
}

updateIP() {
	local node="`echo $1 | sed 's/^0*//'`"
	local ip="`ping -c 1 NAMESCHEME$1 | head -n 1 | cut -d "(" -f 2 | cut -d ")" -f 1`"
	if [ -n "${ips[$node]}" ] && [ "$ip" != "${ips[$node]}" ]; then
		echo "WARNING: IP for NAMESCHEME$1 changed, restarting slurmctld..." | tee -a $LOGFILE
		sudo systemctl restart slurmctld.service
		unset $ip
	fi
	ips[$node]=$ip
}

create() {
	local numToCreate=$1
	existingDisks="`existingDisks`"
	# Determine the next node name to use
	local num
	for num in $(seq -w 1 $MAX_NODE_NUM); do
		if [ "$numToCreate" -le 0 ]; then
			# We're done
			break
		elif [ -z "`state $num`" ]; then
			# This node number is unused
			if [ -n "`grep NAMESCHEME$num <<< $existingDisks`" ]; then
				echo "ERROR: Disk for NAMESCHEME$num exists, but shouldn't! Deleting..." | tee -a $LOGFILE
				echo Y | gcloud compute disks delete NAMESCHEME$num &> /dev/null &
			else
				setState $num C
				sudo scontrol update nodename=NAMESCHEME$num state=resume
				echo "Creating NAMESCHEME$num" | tee -a $LOGFILE
				gcloud compute disks create NAMESCHEME$num --size 10 --source-snapshot NAMESCHEME &> /dev/null &&
				gcloud compute instances create NAMESCHEME$num --machine-type "n1-standard-1" --disk "name=NAMESCHEME$num,device-name=NAMESCHEME$num,mode=rw,boot=yes,auto-delete=yes" &> /dev/null ||
				if [ $? -eq 1 ]; then echo "ERROR: Failed to create NAMESCHEME$num" | tee -a $LOGFILE; fi & #TODO: do something about the failure
				let "numToCreate -= 1"
			fi
		fi
	done
}

delete() {
	local numToDelete=$1
	local idleNodes="`sinfo -o "%t %n" | grep "idle" | awk '{print $2}'`"
	local num
	for num in $(seq -w 1 $MAX_NODE_NUM); do
		if [ "$numToDelete" -le 0 ]; then
			# We're done
			break
		elif [ -n "`grep "NAMESCHEME$num" <<< "$idleNodes"`" ] && [ "`state $num`" == "R" ]; then
			setState $num D
			sudo scontrol update nodename=NAMESCHEME$num state=drain reason="Deleting"
			{
				local n=$num
				while true; do
					if [ -n "`sinfo -h -N -o "%N %t" | grep NAMESCHEME$n | awk '{print $2}' | grep drain`" ]; then
						#It's been drained
						deleteInstance $n
						break
					fi
					sleep 10;
				done
			} &
			let "numToDelete -= 1"
		fi
	done
}

deleteInstance() {
	local num=$1
	echo "Deleting NAMESCHEME$num" | tee -a $LOGFILE
	setState $num D
	sudo scontrol update nodename=NAMESCHEME$num state=down reason="Deleted"
	echo Y | gcloud compute instances delete NAMESCHEME$num &> /dev/null &
}

if [ "`hostname`" == "NAMESCHEME" ]; then
        # do head node stuff
	sudo rm $LOGFILE
	sudo touch $LOGFILE
	sudo chown `whoami`:`whoami` $LOGFILE
        echo "Starting slurmctld.service" | tee -a $LOGFILE
        systemctl restart slurmctld.service
	zone=`gcloud compute instances list | grep "$(hostname) " | awk '{print $2}'`
	echo "Configuring gcloud for zone: $zone" | tee -a $LOGFILE
	gcloud config set compute/zone $zone
	mainLoop &
else
        # do compute node stuff
        #echo "NAMESCHEME:/etc/slurm            /etc/slurm              nfs     ro,hard,intr    0 0" | sudo tee --append /etc/fstab > /dev/null
        #echo "NAMESCHEME:/etc/munge            /etc/munge              nfs     ro,hard,intr    0 0" | sudo tee --append /etc/fstab > /dev/null
        #echo "NAMESCHEME:/home                 /home                   nfs     rw,hard,intr    0 0" | sudo tee --append /etc/fstab > /dev/null
        sudo mount NAMESCHEME:/etc/slurm /etc/slurm --ro
        sudo mount NAMESCHEME:/etc/munge /etc/munge --ro
	sudo mount NAMESCHEME:/home /home --rw
        echo "Starting slurmd.service"
        systemctl start slurmd.service
fi
